"""
AI-Powered Question Generator

Uses OpenAI GPT-5.2 to generate comprehension questions about student code.
Takes static and dynamic analysis results as input and generates contextual,
pedagogically valuable questions.
"""

import os
import json
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from enum import Enum

from openai import OpenAI

# Import analyzers
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from analyzers.static_analyzer import StaticAnalyzer
from analyzers.dynamic_analyzer import DynamicAnalyzer


class QuestionLevel(Enum):
    """Question complexity level based on the Block Model."""
    ATOM = "atom"           # Language elements (variables, operators)
    BLOCK = "block"         # Code sections (loops, functions)
    RELATIONAL = "relational"  # Connections between parts
    MACRO = "macro"         # Whole program understanding


class QuestionType(Enum):
    """Type of question and expected answer format."""
    MULTIPLE_CHOICE = "multiple_choice"
    FILL_IN_BLANK = "fill_in_blank"
    TRUE_FALSE = "true_false"
    SHORT_ANSWER = "short_answer"
    NUMERIC = "numeric"
    CODE_SELECTION = "code_selection"


class AnswerType(Enum):
    """How the answer is determined."""
    STATIC = "static"
    DYNAMIC = "dynamic"
    HYBRID = "hybrid"
    AI = "ai"


@dataclass
class QuestionAnswer:
    """Represents a possible answer to a question."""
    text: str
    is_correct: bool
    explanation: Optional[str] = None


@dataclass
class GeneratedQuestion:
    """A concrete question generated by AI."""
    template_id: str
    question_text: str
    question_type: QuestionType
    question_level: QuestionLevel
    answer_type: AnswerType
    correct_answer: Any
    answer_choices: List[QuestionAnswer] = field(default_factory=list)
    context: Dict[str, Any] = field(default_factory=dict)
    explanation: Optional[str] = None
    difficulty: str = "medium"

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            'template_id': self.template_id,
            'question_text': self.question_text,
            'question_type': self.question_type.value,
            'question_level': self.question_level.value,
            'answer_type': self.answer_type.value,
            'correct_answer': self.correct_answer,
            'answer_choices': [
                {
                    'text': choice.text,
                    'is_correct': choice.is_correct,
                    'explanation': choice.explanation
                }
                for choice in self.answer_choices
            ],
            'context': self.context,
            'explanation': self.explanation,
            'difficulty': self.difficulty
        }


@dataclass
class GenerationConfig:
    """Configuration for question generation."""
    max_questions: int = 10
    min_questions: int = 3
    dynamic_timeout: int = 5

    # Question preferences
    include_levels: List[str] = field(default_factory=lambda: ["atom", "block", "relational", "macro"])
    include_types: List[str] = field(default_factory=lambda: ["multiple_choice", "fill_in_blank", "numeric", "short_answer"])
    include_difficulties: List[str] = field(default_factory=lambda: ["easy", "medium", "hard"])

    # AI settings
    temperature: float = 0.7
    model: str = "gpt-5.2"


@dataclass
class GenerationResult:
    """Result of question generation process."""
    questions: List[GeneratedQuestion]
    static_analysis: Optional[Dict[str, Any]] = None
    dynamic_analysis: Optional[Dict[str, Any]] = None
    total_generated: int = 0
    total_filtered: int = 0
    applicable_templates: int = 0
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    execution_successful: bool = True
    execution_time_ms: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            'questions': [q.to_dict() for q in self.questions],
            'metadata': {
                'total_generated': self.total_generated,
                'total_filtered': self.total_filtered,
                'total_returned': len(self.questions),
                'applicable_templates': self.applicable_templates,
                'execution_successful': self.execution_successful,
                'execution_time_ms': self.execution_time_ms
            },
            'errors': self.errors,
            'warnings': self.warnings,
            'static_analysis_summary': self._summarize_static() if self.static_analysis else None,
            'dynamic_analysis_summary': self._summarize_dynamic() if self.dynamic_analysis else None
        }

    def _summarize_static(self) -> Dict[str, Any]:
        """Create a summary of static analysis."""
        if not self.static_analysis:
            return {}
        return self.static_analysis.get('summary', {})

    def _summarize_dynamic(self) -> Dict[str, Any]:
        """Create a summary of dynamic analysis."""
        if not self.dynamic_analysis:
            return {}
        return {
            'execution_successful': self.dynamic_analysis.get('execution_successful', False),
            'max_stack_depth': self.dynamic_analysis.get('max_stack_depth', 0),
            'total_lines_executed': self.dynamic_analysis.get('total_lines_executed', 0),
            'exception': self.dynamic_analysis.get('exception')
        }


# System prompt for the AI
SYSTEM_PROMPT = """You are an expert computer science educator specializing in assessing student understanding of their own code. Your task is to generate comprehension questions that test whether a student truly understands the code they wrote.

You will receive:
1. The student's Python code
2. Static analysis results (code structure: functions, variables, loops, etc.)
3. Dynamic analysis results (runtime behavior: variable values, loop iterations, function calls, etc.)

Generate questions that:
- Test understanding at different cognitive levels (from basic recall to deep comprehension)
- Have clear, unambiguous correct answers derived from the analysis data
- Cover different aspects of the code (variables, control flow, functions, etc.)
- Are pedagogically valuable - they help identify gaps in understanding

Question Levels (based on Block Model):
- ATOM: Basic language elements (What is the value of X? What type is Y?)
- BLOCK: Code sections (How many times does this loop run? What does this function return?)
- RELATIONAL: Connections between parts (How does function A use the result of function B?)
- MACRO: Whole program understanding (What is the overall purpose of this code?)

Question Types:
- multiple_choice: Provide 4 options, one correct
- fill_in_blank: Student fills in a value
- numeric: Student provides a number
- short_answer: Brief text answer
- true_false: True or False question

IMPORTANT:
- All answers MUST be derivable from the provided analysis data
- For variable values, use EXACT values from dynamic analysis
- For loop counts, use EXACT iteration counts from dynamic analysis
- Never make up or guess values - only use what's in the analysis
- Include the explanation field to help students learn

Return your response as a JSON array of question objects with this structure:
{
  "questions": [
    {
      "template_id": "ai_generated_<type>",
      "question_text": "The question text",
      "question_type": "multiple_choice|fill_in_blank|numeric|short_answer|true_false",
      "question_level": "atom|block|relational|macro",
      "correct_answer": "The correct answer (string or number)",
      "answer_choices": [  // Only for multiple_choice
        {"text": "Option A", "is_correct": false, "explanation": "Why wrong"},
        {"text": "Option B", "is_correct": true, "explanation": "Why correct"},
        ...
      ],
      "difficulty": "easy|medium|hard",
      "explanation": "Educational explanation of the answer",
      "context": {
        "line_number": 5,  // Optional: relevant line number
        "variable_name": "x",  // Optional: relevant variable
        "function_name": "foo"  // Optional: relevant function
      }
    }
  ]
}"""


class AIQuestionGenerator:
    """
    AI-powered question generator using OpenAI GPT-5.2.
    Orchestrates the complete question generation pipeline.
    """

    def __init__(self, config: Optional[GenerationConfig] = None, api_key: Optional[str] = None):
        """
        Initialize the AI question generator.

        Args:
            config: Configuration for question generation
            api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)
        """
        self.config = config or GenerationConfig()
        self.static_analyzer = StaticAnalyzer()
        self.dynamic_analyzer = DynamicAnalyzer(timeout=self.config.dynamic_timeout)

        # Initialize OpenAI client
        self.client = OpenAI(api_key=api_key or os.environ.get("OPENAI_API_KEY"))

    def generate(
        self,
        source_code: str,
        test_inputs: Optional[Dict[str, Any]] = None
    ) -> GenerationResult:
        """
        Generate questions from student code using AI.

        Args:
            source_code: Python source code to analyze
            test_inputs: Optional inputs for dynamic analysis

        Returns:
            GenerationResult with questions and metadata
        """
        start_time = time.time()
        result = GenerationResult(questions=[])

        # Step 1: Static Analysis
        static_analysis = None
        try:
            static_analysis = self.static_analyzer.analyze(source_code)
            result.static_analysis = static_analysis
        except SyntaxError as e:
            result.errors.append(f"Syntax error in code: {e}")
            result.execution_successful = False
            return result
        except Exception as e:
            result.errors.append(f"Static analysis failed: {e}")
            result.warnings.append("Continuing without static analysis")

        # Step 2: Dynamic Analysis
        dynamic_analysis = None
        try:
            dynamic_analysis = self.dynamic_analyzer.analyze(source_code, test_inputs)
            result.dynamic_analysis = dynamic_analysis
            result.execution_successful = dynamic_analysis.get('execution_successful', False)

            if not result.execution_successful:
                exception = dynamic_analysis.get('exception', 'Unknown error')
                result.warnings.append(f"Code execution failed: {exception}")
                result.warnings.append("Questions will be limited to static analysis only")
        except Exception as e:
            result.errors.append(f"Dynamic analysis failed: {e}")
            result.warnings.append("Continuing without dynamic analysis")

        # If both analyses failed, return early
        if not static_analysis and not dynamic_analysis:
            result.errors.append("Both static and dynamic analysis failed")
            return result

        # Step 3: Generate questions using AI
        try:
            questions = self._generate_with_ai(
                source_code,
                static_analysis or {},
                dynamic_analysis
            )
            result.questions = questions
            result.total_generated = len(questions)
            result.applicable_templates = 1  # AI acts as one "template"
        except Exception as e:
            result.errors.append(f"AI question generation failed: {e}")
            return result

        # Calculate execution time
        end_time = time.time()
        result.execution_time_ms = (end_time - start_time) * 1000

        return result

    def _generate_with_ai(
        self,
        source_code: str,
        static_analysis: Dict[str, Any],
        dynamic_analysis: Optional[Dict[str, Any]]
    ) -> List[GeneratedQuestion]:
        """
        Call OpenAI API to generate questions.

        Args:
            source_code: The original source code
            static_analysis: Results from static analyzer
            dynamic_analysis: Results from dynamic analyzer

        Returns:
            List of generated questions
        """
        # Build the user prompt with all context
        user_prompt = self._build_prompt(source_code, static_analysis, dynamic_analysis)

        # Call OpenAI API
        response = self.client.chat.completions.create(
            model=self.config.model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=self.config.temperature,
            response_format={"type": "json_object"}
        )

        # Parse the response
        response_text = response.choices[0].message.content
        response_data = json.loads(response_text)

        # Convert to GeneratedQuestion objects
        questions = []
        for q_data in response_data.get("questions", []):
            try:
                question = self._parse_question(q_data)
                if question:
                    questions.append(question)
            except Exception as e:
                # Skip malformed questions
                continue

        # Apply limits
        if len(questions) > self.config.max_questions:
            questions = questions[:self.config.max_questions]

        return questions

    def _build_prompt(
        self,
        source_code: str,
        static_analysis: Dict[str, Any],
        dynamic_analysis: Optional[Dict[str, Any]]
    ) -> str:
        """Build the prompt for the AI with all analysis context."""

        prompt_parts = [
            "## Student's Code\n```python",
            source_code,
            "```\n",
            "## Static Analysis Results",
            json.dumps(static_analysis, indent=2, default=str),
            "\n"
        ]

        if dynamic_analysis:
            prompt_parts.extend([
                "## Dynamic Analysis Results (Runtime Behavior)",
                json.dumps(dynamic_analysis, indent=2, default=str),
                "\n"
            ])
        else:
            prompt_parts.append("## Dynamic Analysis: Not available (code did not execute successfully)\n")

        prompt_parts.extend([
            f"## Requirements",
            f"- Generate {self.config.max_questions} questions",
            f"- Include question levels: {', '.join(self.config.include_levels)}",
            f"- Include question types: {', '.join(self.config.include_types)}",
            f"- Include difficulties: {', '.join(self.config.include_difficulties)}",
            "",
            "Generate diverse, pedagogically valuable questions based on the code and analysis above.",
            "Remember: All answers must be derived from the actual analysis data provided."
        ])

        return "\n".join(prompt_parts)

    def _parse_question(self, q_data: Dict[str, Any]) -> Optional[GeneratedQuestion]:
        """Parse a question from the AI response."""

        # Map string values to enums
        question_type_map = {
            "multiple_choice": QuestionType.MULTIPLE_CHOICE,
            "fill_in_blank": QuestionType.FILL_IN_BLANK,
            "true_false": QuestionType.TRUE_FALSE,
            "short_answer": QuestionType.SHORT_ANSWER,
            "numeric": QuestionType.NUMERIC,
            "code_selection": QuestionType.CODE_SELECTION,
        }

        question_level_map = {
            "atom": QuestionLevel.ATOM,
            "block": QuestionLevel.BLOCK,
            "relational": QuestionLevel.RELATIONAL,
            "macro": QuestionLevel.MACRO,
        }

        q_type = question_type_map.get(q_data.get("question_type", "").lower())
        q_level = question_level_map.get(q_data.get("question_level", "").lower())

        if not q_type or not q_level:
            return None

        # Parse answer choices for multiple choice
        answer_choices = []
        if q_type == QuestionType.MULTIPLE_CHOICE:
            for choice_data in q_data.get("answer_choices", []):
                choice = QuestionAnswer(
                    text=str(choice_data.get("text", "")),
                    is_correct=choice_data.get("is_correct", False),
                    explanation=choice_data.get("explanation")
                )
                answer_choices.append(choice)

        return GeneratedQuestion(
            template_id=q_data.get("template_id", "ai_generated"),
            question_text=q_data.get("question_text", ""),
            question_type=q_type,
            question_level=q_level,
            answer_type=AnswerType.AI,
            correct_answer=q_data.get("correct_answer"),
            answer_choices=answer_choices,
            context=q_data.get("context", {}),
            explanation=q_data.get("explanation"),
            difficulty=q_data.get("difficulty", "medium")
        )


# Convenience functions

def generate_questions(
    source_code: str,
    config: Optional[GenerationConfig] = None,
    test_inputs: Optional[Dict[str, Any]] = None,
    api_key: Optional[str] = None
) -> GenerationResult:
    """
    Convenience function to generate questions from code.

    Args:
        source_code: Python source code
        config: Optional generation configuration
        test_inputs: Optional inputs for dynamic analysis
        api_key: Optional OpenAI API key

    Returns:
        GenerationResult with questions and metadata
    """
    generator = AIQuestionGenerator(config, api_key)
    return generator.generate(source_code, test_inputs)


def generate_questions_simple(
    source_code: str,
    max_questions: int = 10,
    api_key: Optional[str] = None
) -> List[GeneratedQuestion]:
    """
    Simplest interface - just get questions.

    Args:
        source_code: Python source code
        max_questions: Maximum number of questions to return
        api_key: Optional OpenAI API key

    Returns:
        List of generated questions
    """
    config = GenerationConfig(max_questions=max_questions)
    result = generate_questions(source_code, config, api_key=api_key)
    return result.questions
